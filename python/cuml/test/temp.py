from hmmlearn import _hmmc

from hmmlearn.utils import normalize, log_normalize, iter_from_X_lengths, log_mask_zero

import numpy as np

def _accumulate_sufficient_statistics(self, stats, X, framelogprob,
                                      posteriors, fwdlattice, bwdlattice):
    """Updates sufficient statistics from a given sample.

    Parameters
    ----------
    stats : dict
        Sufficient statistics as returned by
        :meth:`~base._BaseHMM._initialize_sufficient_statistics`.

    X : array, shape (n_samples, n_features)
        Sample sequence.

    framelogprob : array, shape (n_samples, n_components)
        Log-probabilities of each sample under each of the model states.

    posteriors : array, shape (n_samples, n_components)
        Posterior probabilities of each sample being generated by each
        of the model states.

    fwdlattice, bwdlattice : array, shape (n_samples, n_components)
        Log-forward and log-backward probabilities.
    """
    stats['nobs'] += 1
    if 's' in self.params:
        stats['start'] += posteriors[0]
    if 't' in self.params:
        n_samples, n_components = framelogprob.shape
        # when the sample is of length 1, it contains no transitions
        # so there is no reason to update our trans. matrix estimate
        if n_samples <= 1:
            return

        log_xi_sum = np.full((n_components, n_components), -np.inf)
        _hmmc._compute_log_xi_sum(n_samples, n_components, fwdlattice,
                                  log_mask_zero(self.transmat_),
                                  bwdlattice, framelogprob,
                                  log_xi_sum)
        with np.errstate(under="ignore"):
            stats['trans'] += np.exp(log_xi_sum)
        print("log_xi_sum", log_xi_sum)
        print("stats", stats['trans'])


def _do_mstep(self, stats):
    """Performs the M-step of EM algorithm.

    Parameters
    ----------
    stats : dict
        Sufficient statistics updated from all available samples.
    """
    # The ``np.where`` calls guard against updating forbidden states
    # or transitions in e.g. a left-right HMM.
    if 's' in self.params:
        startprob_ = self.startprob_prior - 1.0 + stats['start']
        self.startprob_ = np.where(self.startprob_ == 0.0,
                                   self.startprob_, startprob_)
        normalize(self.startprob_)
    if 't' in self.params:
        transmat_ = self.transmat_prior - 1.0 + stats['trans']
        self.transmat_ = np.where(self.transmat_ == 0.0,
                                  self.transmat_, transmat_)
        normalize(self.transmat_, axis=1)


def fit(self, X, lengths=None):
    """Estimate model parameters.

    An initialization step is performed before entering the
    EM algorithm. If you want to avoid this step for a subset of
    the parameters, pass proper ``init_params`` keyword argument
    to estimator's constructor.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Feature matrix of individual samples.

    lengths : array-like of integers, shape (n_sequences, )
        Lengths of the individual sequences in ``X``. The sum of
        these should be ``n_samples``.

    Returns
    -------
    self : object
        Returns self.
    """

    self._init(X, lengths=lengths)
    self._check()

    # self.monitor_._reset()
    for iter in range(self.n_iter):
        stats = self._initialize_sufficient_statistics()
        curr_logprob = 0
        for i, j in iter_from_X_lengths(X, lengths):
            framelogprob = self._compute_log_likelihood(X[i:j])
            logprob, fwdlattice = self._do_forward_pass(framelogprob)
            curr_logprob += logprob
            bwdlattice = self._do_backward_pass(framelogprob)
            posteriors = self._compute_posteriors(fwdlattice, bwdlattice)
            _accumulate_sufficient_statistics(self,
                stats, X[i:j], framelogprob, posteriors, fwdlattice,
                bwdlattice)

        # XXX must be before convergence check, because otherwise
        #     there won't be any updates for the case ``n_iter=1``.
        _do_mstep(self, stats)

        # self.monitor_.report(curr_logprob)
        # if self.monitor_.converged:
        #     break

    return self

def score(self, X, lengths=None):
    """Compute the log probability under the model.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Feature matrix of individual samples.

    lengths : array-like of integers, shape (n_sequences, ), optional
        Lengths of the individual sequences in ``X``. The sum of
        these should be ``n_samples``.

    Returns
    -------
    logprob : float
        Log likelihood of ``X``.

    See Also
    --------
    score_samples : Compute the log probability under the model and
        posteriors.
    decode : Find most likely state sequence corresponding to ``X``.
    """
    self._check()

    # XXX we can unroll forward pass for speed and memory efficiency.
    probs = []
    logprob = 0
    for i, j in iter_from_X_lengths(X, lengths):
        framelogprob = self._compute_log_likelihood(X[i:j])
        logprobij, _fwdlattice = self._do_forward_pass(framelogprob)
        logprob += logprobij
        probs.append(logprobij)
    return logprob, np.array(probs)


def decode(self, X, lengths=None, algorithm=None):
    """Find most likely state sequence corresponding to ``X``.

    Parameters
    ----------
    X : array-like, shape (n_samples, n_features)
        Feature matrix of individual samples.

    lengths : array-like of integers, shape (n_sequences, ), optional
        Lengths of the individual sequences in ``X``. The sum of
        these should be ``n_samples``.

    algorithm : string
        Decoder algorithm. Must be one of "viterbi" or "map".
        If not given, :attr:`decoder` is used.

    Returns
    -------
    logprob : float
        Log probability of the produced state sequence.

    state_sequence : array, shape (n_samples, )
        Labels for each sample from ``X`` obtained via a given
        decoder ``algorithm``.

    See Also
    --------
    score_samples : Compute the log probability under the model and
        posteriors.
    score : Compute the log probability under the model.
    """

    self._check()

    algorithm = algorithm or self.algorithm
    decoder = {
        "viterbi": self._decode_viterbi,
        "map": self._decode_map
    }[algorithm]

    n_samples = X.shape[0]
    logprob = 0
    state_sequence = np.empty(n_samples, dtype=int)
    probs = []

    for i, j in iter_from_X_lengths(X, lengths):
        # XXX decoder works on a single sample at a time!
        logprobij, state_sequenceij = decoder(X[i:j])
        logprob += logprobij
        state_sequence[i:j] = state_sequenceij
        probs.append(logprobij)

    return logprob, state_sequence, np.array(probs)